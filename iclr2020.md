---
marp: true
---

# **G**enerative **Ra**tio **M**atching Networks

Akash Srivastava$^{\ast,1,2,3}$, Kai Xu$^{\ast,3}$, Michael U. Gutmann$^{3}$, Charles Sutton$^{3,4,5}$

<br>

$\ast$ equal contributions
$^1$MIT-IBM Watson AI Lab $^2$IBM Research $^3$University of Edinburgh $^4$Google AI  $^5$Alan Turing Institute

---

## Introduction


Adversarial Generative Models(GANs, MMD-GANs)
:white_check_mark: can generate high-dimensional data such as natural images. 
:x: are very difficult to train due to the saddlepoint optimization problem

GRaM is a *stable* learning algorithm for *implicit* deep generative models that does __not__ invlove a saddlepoint optimization problem and therefore is easy to train :tada:



---

## Overview

1. Learn a low-dimensional manifold
    * that preserves the difference between the data ($p_x$) and the model ($q_x$) densities.
    * We use the ratio ($r(x) = \frac{p_x}{q_x}$) of the two densities as the measure of this difference.
2. Train the model ($G_\gamma$) in the low-dimensional manifold
    * using the *Maximum Mean Discrepency* criterion as it work very well in low dimensional data.

---

## GRaM: Algorithm

1. Learn the manifold projection function $f_\theta(x)$ by minimising the squared difference between the pair of density ratios:

$$
\begin{aligned}
D(\theta) 
&= \int q_x(x) \left( \frac{p_x(x)}{q_x(x)} - \frac{\bar{p}(f_\theta(x))}{\bar{q}(f_\theta(x))} \right)^2 dx \\
&= C - \mathrm{PD}(\bar{q}, \bar{p})
\end{aligned}
$$

<!-- We can *minimise* the squared difference by *maximizing* Pearson Divergence in the low dimensional space :heart: -->

---

2. Train the generator $G\gamma$ by minimizing the empirical estimator of MMD in the low-dimensional manifold, 

$$
\begin{aligned}
\min_\gamma \Bigg[&\frac{1}{N^2}\sum_{i=1}^N\sum_{i'=1}^N k(f_\theta(x_i),f_\theta(x_{i'})) 
- \frac{2}{NM}\sum_{i=1}^N\sum_{j=1}^M  k(f_\theta(x_i), f_\theta(G_\gamma(z_j)))\\
&\quad + \frac{1}{M^2}\sum_{j=1}^M\sum_{j'=1}^M k(f_\theta(G_\gamma(z_j)),f_\theta(G_\gamma(z_{j'}))) \Bigg ]
\end{aligned}
$$

---

## Pearson Divergence Maximisation and Density Ratio Estimation

Monte Carlo approximation of PD,
$$
\mathrm{PD}(\bar{q}, \bar{p}) \approx \frac{1}{N} \sum_{i=1}^N \left( \frac{\bar{p}(f_\theta(x_i))}{\bar{q}(f_\theta(x_i))} \right)^2 - 1 
$$
where $x^q_i \sim q_x$.

<!-- For this to work, we need an estimator of the density ratio. -->
<!-- - We only need density ratios $\frac{\bar{p}(f_\theta(x))}{\bar{q}(f_\theta(x))}$ for a set of samples from $q$ during MC. -->
We use a MMD based density ratio estimator (Sugiyama et al., 2012) under the fixed-design setup: $\hat{r}_q = \mathbf{K}^{-1}_{q,q} \mathbf{K}_{q,p}\mathbf{1}$.
- $\mathbf{K}_{q,q}$ and $\mathbf{K}_{q,p}$ are Gram matrices defined by $[\mathbf{K}_{q,q}]_{i,j} = k(f_\theta(x^q_i),f_\theta(x^q_j))$ and $[\mathbf{K}_{q,p}]_{i,j} = k(f_\theta(x^q_i),f_\theta(x^p_j)).$
<!-- - Train the generator via the MMD loss -->
<!-- - Shared Gram matrix between density ratio estimation and generator training
- Simultaneous training of the transform function and the generator -->



---
## The Ring dataset: Illustration of the Method and Stability

![](syn_0.png)

---

![](syn_1.png)

---

![](syn_2.png)

---

## Quantitative Results: Sample Quality

![](table_image.png)

---
## Qualitative Results: Random Samples

![](cifar10.png) ![](celeba.png)

---

# The End!
Extra slides to follow...

---

## Density Ratio Estimation via (Infinite) Moment Matching

*Maximum mean discrepancy*

$$
\textrm{MMD}_{\mathcal{F}}(p,q) = \sup_{f\in\mathcal{F}} \left(\mathbb{E}_p \lbrack f(x) \rbrack - \mathbb{E}_q \lbrack f(x) \rbrack \right)
$$

Gretton et al. (2012) show that it is sufficient to choose $\mathcal{F}$ to be a unit ball in an reproducing kernel Hilbert space $\mathcal{R}$ with a characteristic kernel $k$.

<!-- $$
\hat{\textmd{MMD}}^2_\mathcal{R}(p,q) =
\frac{1}{N^2}\sum_{i=1}^N\sum_{i'=1}^N k(x_i,x_{i'}) 
- \frac{2}{NM}\sum_{i=1}^N\sum_{j=1}^M  k(x_i, y_j)
 + \frac{1}{M^2}\sum_{j=1}^M\sum_{j'=1}^M k(y_j,y_{j'})
$$ -->
Using this definition of MMD, the density ratio estimator $r(x)$ can be derived as the solution to
$$
\min_{r\in\mathcal{R}} \bigg \Vert \int k(x; .)p(x) dx - \int k(x; .)r(x)q(x) dx \bigg \Vert_{\mathcal{R}}^2.
$$

---

## Generator Training

- The generator $G\gamma$ is trained by minimizing the empirical estimator of MMD, 

$$
\begin{aligned}
\min_\gamma \Bigg[&\frac{1}{N^2}\sum_{i=1}^N\sum_{i'=1}^N k(f_\theta(x_i),f_\theta(x_{i'})) 
- \frac{2}{NM}\sum_{i=1}^N\sum_{j=1}^M  k(f_\theta(x_i), f_\theta(G_\gamma(z_j)))\\
&\quad + \frac{1}{M^2}\sum_{j=1}^M\sum_{j'=1}^M k(f_\theta(G_\gamma(z_j)),f_\theta(G_\gamma(z_{j'}))) \Bigg ]
\end{aligned}
$$

with respect to it's parameters $\gamma$.